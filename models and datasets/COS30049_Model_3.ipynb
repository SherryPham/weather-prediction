{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL 3 - Weather Pattern Analysis (Clustering)"
      ],
      "metadata": {
        "id": "2Cdf4151apWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "yKX7BJN5Omoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load data"
      ],
      "metadata": {
        "id": "JJEN-YhmWKYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Firstly, we load the melbourne_weather.csv data file which is the result of our data pre-processing part.\n",
        "\n",
        "*   To get this data file, please refer to our Data Processing notebook's instructions.\n",
        "\n",
        "*   Next we display the first few rows and data info."
      ],
      "metadata": {
        "id": "CNI7Nga-eoon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "origin_df = pd.read_csv('melbourne_weather.csv')\n",
        "\n",
        "# Display the first few rows and data info\n",
        "print(origin_df.head())\n",
        "print(origin_df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZvRWSWsOp1p",
        "outputId": "452db550-a184-4cf3-da1f-f2f7e000711f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Date, Location, Rainfall, Evaporation, Sunshine, WindGustDir, WindGustSpeed, WindDir9am, WindDir3pm, WindSpeed9am, WindSpeed3pm, RainToday, RainTomorrow, tempmax, tempmin, temp, feelslikemax, feelslikemin, feelslike, dew, humidity, precip, precipprob, precipcover, sealevelpressure, cloudcover, visibility, solarradiation, solarenergy, uvindex, sunrise, sunset, moonphase, TomorrowTemp]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 34 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 0 entries\n",
            "Data columns (total 34 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   Date              0 non-null      object\n",
            " 1   Location          0 non-null      object\n",
            " 2   Rainfall          0 non-null      object\n",
            " 3   Evaporation       0 non-null      object\n",
            " 4   Sunshine          0 non-null      object\n",
            " 5   WindGustDir       0 non-null      object\n",
            " 6   WindGustSpeed     0 non-null      object\n",
            " 7   WindDir9am        0 non-null      object\n",
            " 8   WindDir3pm        0 non-null      object\n",
            " 9   WindSpeed9am      0 non-null      object\n",
            " 10  WindSpeed3pm      0 non-null      object\n",
            " 11  RainToday         0 non-null      object\n",
            " 12  RainTomorrow      0 non-null      object\n",
            " 13  tempmax           0 non-null      object\n",
            " 14  tempmin           0 non-null      object\n",
            " 15  temp              0 non-null      object\n",
            " 16  feelslikemax      0 non-null      object\n",
            " 17  feelslikemin      0 non-null      object\n",
            " 18  feelslike         0 non-null      object\n",
            " 19  dew               0 non-null      object\n",
            " 20  humidity          0 non-null      object\n",
            " 21  precip            0 non-null      object\n",
            " 22  precipprob        0 non-null      object\n",
            " 23  precipcover       0 non-null      object\n",
            " 24  sealevelpressure  0 non-null      object\n",
            " 25  cloudcover        0 non-null      object\n",
            " 26  visibility        0 non-null      object\n",
            " 27  solarradiation    0 non-null      object\n",
            " 28  solarenergy       0 non-null      object\n",
            " 29  uvindex           0 non-null      object\n",
            " 30  sunrise           0 non-null      object\n",
            " 31  sunset            0 non-null      object\n",
            " 32  moonphase         0 non-null      object\n",
            " 33  TomorrowTemp      0 non-null      object\n",
            "dtypes: object(34)\n",
            "memory usage: 124.0+ bytes\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Season Clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "oz3pm9MoI87S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  Data need to be standardized to ensure all features contribute equally to the K means clustering.\n",
        "\n",
        "*   **Feature Selection**: Select important features such as temperature, humidity, evaporation, cloud cover, and rainfall to identify different weather patterns.\n",
        "*   **Data Cleaning**: Remove rows with missing values using (.dropna()) to ensure the dataset is ready for accurate analysis.\n",
        "\n",
        "\n",
        "*   **Standardization**: Use StandardScaler() to scale the features so that variables like temperature (which have large range value) don't dominate others like rainfall (which have smaller values).\n",
        "\n"
      ],
      "metadata": {
        "id": "FkqJcemjJudV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features for clustering\n",
        "features = ['temp', 'humidity', 'Evaporation', 'cloudcover', 'Rainfall']\n",
        "# features = ['temp', 'Evaporation', 'precip']\n",
        "df = origin_df[['Date'] + features]\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Display the cleaned dataset\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbN0PfGW5CYc",
        "outputId": "9dc5533e-8d91-4c32-d941-ccec28f131f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 0 entries\n",
            "Data columns (total 6 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   Date         0 non-null      object\n",
            " 1   temp         0 non-null      object\n",
            " 2   humidity     0 non-null      object\n",
            " 3   Evaporation  0 non-null      object\n",
            " 4   cloudcover   0 non-null      object\n",
            " 5   Rainfall     0 non-null      object\n",
            "dtypes: object(6)\n",
            "memory usage: 124.0+ bytes\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-1e987330e829>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.dropna(inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data\n",
        "X = df[features]\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Display the scaled features\n",
        "print(pd.DataFrame(X_scaled, columns=features).head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "PgCPaHea5KUc",
        "outputId": "96f93f64-b6c8-4e09-983a-cee358d4958f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found array with 0 sample(s) (shape=(0, 5)) while a minimum of 1 is required by StandardScaler.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f3eb3801ddc3>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Normalize the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Display the scaled features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_fit_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer_skip_nested_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \"\"\"\n\u001b[1;32m    913\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1088\u001b[0m                 \u001b[0;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m                 \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 5)) while a minimum of 1 is required by StandardScaler."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 KMeans"
      ],
      "metadata": {
        "id": "QNwuKUemD6F8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   KMeans is a clustering algorithms commonly used for its efficient, easy to implement, and provides clear, well-separated clusters based on distance between data points with large dataset.\n",
        "\n",
        "*   **KMeans Algorithm**: Set n_clusters=3 to create 3 groups of weather patterns. This is an arbitrary choice, the model can be improved later based on evaluation metrics ( Elbow Method).\n",
        "*   **Result**: Each data point is assigned to a cluster (cluster 0, 1, or 2). These clusters represent different types of weather patterns based on avarage value group ( hot and dry, cool and humid).\n",
        "\n",
        "*   **Visualization**: We use sns.pairplot() to visualize the clusters, providing understand on how the data points are grouped.\n",
        "\n",
        "\n",
        "*   **Cluster Centers**: Heatmap is used to visualize the average weather conditions in each cluster. This helps interpreting the property of each weather cluster group.\n",
        "\n"
      ],
      "metadata": {
        "id": "PM4w_MKgKRtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform K-means clustering\n",
        "n_clusters = 3  # You can adjust this number\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "df['Cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Visualize the clusters by mapping on a pair plot of different features\n",
        "plt.figure(figsize=(15, 15))\n",
        "sns.pairplot(df[features + ['Cluster']], hue='Cluster', palette='viridis')\n",
        "plt.suptitle('Weather Clusters Visualization', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize cluster centers\n",
        "cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
        "center_df = pd.DataFrame(cluster_centers, columns=features)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(center_df, annot=True, cmap='YlGnBu', fmt='.2f')\n",
        "plt.title('Cluster Centers')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kVFr41fC1irN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 DBScan (Optional)"
      ],
      "metadata": {
        "id": "PUT_Qhm8D_dY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   DBSCAN is a density-based clustering algorithm that can be used when the number of cluster is unknown. It als handle noise in unpredictability data effectively, which is very common in data weather.\n",
        "\n",
        "*   **DBSCAN Algorithm**: Instead of specifying the number of clusters, DBSCAN finds clusters based on how closely packed the data points are. The model can also detect noise points, which donâ€™t fit into any cluster.\n",
        "*   **Result**: Analyze how many clusters are found and how many points are considered noise. This is usually better when analyzing pattern with unpredicatble weather data where some value might not belong into any specific group.\n",
        "\n",
        "\n",
        "*   **Visualization**: Visualize the DBSCAN clusters using a pair plot to see how data points are grouped.\n"
      ],
      "metadata": {
        "id": "TwY0jOyuKzTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Perform DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)  # You can adjust eps and min_samples\n",
        "df['DBSCAN_Cluster'] = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# Visualize the clusters using a pair plot\n",
        "plt.figure(figsize=(15, 15))\n",
        "sns.pairplot(df[features + ['DBSCAN_Cluster']], hue='DBSCAN_Cluster', palette='viridis')\n",
        "plt.suptitle('Weather Clusters Visualization (DBSCAN)', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze the number of clusters and noise points\n",
        "n_clusters_dbscan = len(set(df['DBSCAN_Cluster'])) - (1 if -1 in df['DBSCAN_Cluster'] else 0)\n",
        "n_noise_dbscan = list(df['DBSCAN_Cluster']).count(-1)\n",
        "\n",
        "print(f'Number of clusters (DBSCAN): {n_clusters_dbscan}')\n",
        "print(f'Number of noise points (DBSCAN): {n_noise_dbscan}')\n"
      ],
      "metadata": {
        "id": "wwSjiO4DEGcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Finalize"
      ],
      "metadata": {
        "id": "5h3nRD5nELMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   Based on K means clustering plot, analyze the characteristics of each cluster. This helps in interpreting what each cluster represents in terms of weather patterns.\n",
        "*   **Cluster Mean Analysis**: Calculate the average values for temperature, humidity, evaporation, cloud cover, and rainfall in each cluster. This value provide a summary of typical weather conditions and value to expect in each group.\n",
        "*   **Temporal Analysis**: Use a plot to show the distribution of clusters across different months, we can see if certain weather patterns (e.g., hot and dry days) are more common in specific seasons.\n",
        "\n"
      ],
      "metadata": {
        "id": "zb_eh2PXLQr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze cluster characteristics\n",
        "print(\"Cluster Characteristics:\")\n",
        "for i in range(n_clusters):\n",
        "    cluster_data = df[df['Cluster'] == i]\n",
        "    print(f\"\\nCluster {i}:\")\n",
        "    print(cluster_data[features].mean())\n",
        "\n",
        "# Temporal analysis\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Month'] = df['Date'].dt.month\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=df, x='Month', hue='Cluster', palette='viridis')\n",
        "plt.title('Distribution of Clusters Across Months')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Cluster')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kLdGkUkG5yOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This clustering approach can help identify different weather patterns or \"types of days\" in Melbourne based on the selected features. For example, you might find clusters representing:\n",
        "\n",
        "* Hot and dry days\n",
        "* Cool and humid days\n",
        "* Moderate weather days\n",
        "\n",
        "The visualizations will help you understand how these clusters are distributed and how they relate to different weather variables. The temporal analysis can reveal if certain weather patterns are more common in specific months."
      ],
      "metadata": {
        "id": "6LCTQdYI1tOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Evaluation"
      ],
      "metadata": {
        "id": "ujUfsiyLWWXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Elbow Method\n",
        "\n",
        "*   **Purpose**: Value to find the optimal number of clusters in KMeans.\n",
        "*   **Explanation**: We plot the inertia for different numbers of clusters. The \"elbow\" point indicates the best number, where adding more clusters no longer significantly improves the fit.\n",
        "\n",
        "\n",
        "2.   Silhouette Score\n",
        "\n",
        "*   **Purpose**: To measure how well-separated the clusters are.\n",
        "*   **Explanation**: A higher Silhouette Score indicates clearer cluster boundaries.\n",
        "\n",
        "\n",
        "\n",
        "3.   Davies-Bouldin Index\n",
        "\n",
        "*   **Purpose**: To assess cluster separation quality.\n",
        "*   **Explanation**: A lower Davies-Bouldin score means better-defined clusters.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q-pYadfbLf22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to calculate Elbow Method, Silhouette Score, and Davies-Bouldin Index\n",
        "def evaluate_clustering(X_scaled, max_clusters=10):\n",
        "    inertia_values = []\n",
        "    silhouette_scores = []\n",
        "    davies_bouldin_scores = []\n",
        "\n",
        "    # Evaluate k-means clustering for different number of clusters\n",
        "    for k in range(2, max_clusters + 1):\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "        # Calculate inertia (within-cluster sum of squares)\n",
        "        inertia_values.append(kmeans.inertia_)\n",
        "\n",
        "        # Calculate silhouette score\n",
        "        silhouette_avg = silhouette_score(X_scaled, labels)\n",
        "        silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "        # Calculate Davies-Bouldin index\n",
        "        db_score = davies_bouldin_score(X_scaled, labels)\n",
        "        davies_bouldin_scores.append(db_score)\n",
        "\n",
        "    # Plot Elbow Method (inertia vs. number of clusters)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(2, max_clusters + 1), inertia_values, marker='o', linestyle='--')\n",
        "    plt.title('Elbow Method for Optimal k')\n",
        "    plt.xlabel('Number of Clusters (k)')\n",
        "    plt.ylabel('Inertia')\n",
        "    plt.xticks(range(2, max_clusters + 1))  # Ensure all k values are shown\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Silhouette Scores\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(2, max_clusters + 1), silhouette_scores, marker='o', linestyle='--', color='green')\n",
        "    plt.title('Silhouette Scores vs Number of Clusters')\n",
        "    plt.xlabel('Number of Clusters (k)')\n",
        "    plt.ylabel('Silhouette Score')\n",
        "    plt.xticks(range(2, max_clusters + 1))\n",
        "    plt.axhline(y=max(silhouette_scores), color='red', linestyle='--', label='Max Silhouette Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Davies-Bouldin Index\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(2, max_clusters + 1), davies_bouldin_scores, marker='o', linestyle='--', color='red')\n",
        "    plt.title('Davies-Bouldin Index vs Number of Clusters')\n",
        "    plt.xlabel('Number of Clusters (k)')\n",
        "    plt.ylabel('Davies-Bouldin Index')\n",
        "    plt.xticks(range(2, max_clusters + 1))\n",
        "    plt.axhline(y=min(davies_bouldin_scores), color='blue', linestyle='--', label='Min Davies-Bouldin Index')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Call the evaluation function for a specific season\n",
        "evaluate_clustering(X_scaled, max_clusters=10)\n"
      ],
      "metadata": {
        "id": "8SxUIAavXr9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Save the model"
      ],
      "metadata": {
        "id": "n-4DmgIr4m9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the model to a file\n",
        "filename = 'model_3.pkl'\n",
        "pickle.dump(kmeans, open(filename, 'wb'))\n",
        "\n",
        "print(f\"Model saved to {filename}\")"
      ],
      "metadata": {
        "id": "_TzG53gI4oEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9fzpPuGGoq6R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}